<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>Regularization from the Spectral Perspective with Dropout</title>
      <meta property="og:title" content="Regularization from the Spectral Perspective with Dropout" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Regularization from the Spectral Perspective with Dropout</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Emily Gan</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Andrew Kessler</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
				<!-- table of contents here -->
				<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
					<b style="font-size:16px">Outline</b><br><br>
					<a href="#intro">Introduction</a><br><br>
					<a href="#spectral_dropout">Spectral Dropout</a><br><br>
					<a href="#experiments">Experiments</a><br><br>
					<a href="#next_steps_and_considerations">Next Steps and Considerations</a><br><br>
					<a href="#conclusion">Conclusion</a><br><br>
				</div>
			</div>

		    <div class="main-content-block">
				<!--You can embed an image like this:-->
				<img src="./images/intro.png" width=800px/>
				<figcaption>Figure 1: Spectral Dropout randomly zeros out some spectral components in deep nets' linear layers for regularization.</figcaption>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

    <div class="content-margin-container" id="Introduction">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Introduction</h2>
			A central problem in deep learning is that of regularization. Vanilla models often are overfit to the training data, and regularization is added to the system in order to improve generalization. Broadly, this is done on the data-side (e.g. augmentation), in model structure (layer norm or dropout), and/or in the loss function itself (norm regularization). There are many arguments for regularization in favoring these simpler models, but the question is always <em>simple in what sense</em>? For example, L2 regularization promotes small solutions, but L1 norm regularization is better for when sparsity is needed. Modern data science practitioners must be able to identify when and where to apply different regularization techniques. Here  we perform regularization from a
			spectral perspective and introduce a new technique we call <em>spectral dropout</em>.
			<br><br>
			<h3>What's the Spectral Perspective?</h3>
			Take a standard multi-layer perceptron (MLP) model. This consists of many linear layers intertwined with pointwise non-linearities. Since the input and output to any particular layer can be represented as vectors, we can represent the layerâ€™s mapping from inputs to outputs as a weight matrix <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math>. It turns out that penalizing the complexity of these layer matrices can help prevent overfitting in neural nets.
			<br><br>
			How does one measure the size or complexity of a matrix? Any matrix can be simplified to a scaled sum of smaller rank-one matrices via its <em>singular value decomposition</em> (SVD). This decomposition produces bases that span its column space as well as feature space, and correpsonding importances of these singular vectors. The SVD is essential for data compression and dimensionality reduction, hence why it naturally lends itself to regularization in deep learning. It's been deployed in several data science applications like recommender systems, latent semantic analysis, and noise filtering <a href="#ref_0">[0]</a>. So in an arbitrary matrix of rank <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math> we have the following SVD:
			<br><br>
			<div style="text-align:center;">
    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
        <mrow>
            <mi>W</mi>
            <mo>=</mo>
            <mi>U</mi>
            <mo>&#x03A3;</mo>
            <msup>
                <mi>V</mi>
                <mo>T</mo>
            </msup>
        </mrow>
		<mrow> <mo>=</mo> </mrow>
		<mrow>
      <msub>
        <mi>Ïƒ</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>u</mi>
        <mn>1</mn>
      </msub>
      <msup>
        <msub>
          <mi>v</mi>
          <mn>1</mn>
        </msub>
        <mo>âŠ¤</mo>
      </msup>
      <mo>+</mo>
      <msub>
        <mi>Ïƒ</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>u</mi>
        <mn>2</mn>
      </msub>
      <msup>
        <msub>
          <mi>v</mi>
          <mn>2</mn>
        </msub>
        <mo>âŠ¤</mo>
      </msup>
      <mo>+</mo>
      <mo>â‹¯</mo>
      <mo>+</mo>
      <msub>
        <mi>Ïƒ</mi>
        <mi>k</mi>
      </msub>
      <msub>
        <mi>u</mi>
        <mi>k</mi>
      </msub>
      <msup>
        <msub>
          <mi>v</mi>
          <mi>k</mi>
        </msub>
        <mo>âŠ¤</mo>
      </msup>
    </mrow>
    </math>
			</div>
			<br><br>
			<div class="centered-image">
				<img src="./images/svd.png" width=512px/>
				<figcaption>Figure 2: The Singular Value Decomposition.</figcaption>
			</div>
			<br><br>
			A common measure of complexity for a matrix is its spectral norm, which is largest of its singular values <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x03C3;</mi><mo>(</mo><mi>W</mi><mo>)</mo><mo>=</mo><mo>max</mo><msub><mi>&#x03C3;</mi><mi>i</mi></msub></math>. Yoshida et al. <a href="#ref_1">[1]</a> showed that penalizing the size of weight matrices via the spectral norm improved generalization of deep learning models. Moreover, they generated adversarial inputs that were perturbed only slightly in spectral norm, which upon training with this augmented data again reduced overfitting. In this blog we also regularize from a spectral perspective, where we aim to limit the complexity of spectral decompositions.
			<br><br>
			<h3>Dropout</h3>
			Dropout is a regularization technique introduced by Srivastava et al. <a href="#ref_2">[2]</a> to prevent overfitting in neural networks. The method works by randomly setting a fraction of the neurons' activations to zero during each forward pass in training. This stochastic behavior forces the network to learn robust features that generalize well to unseen data. If there's always some chance of a neuron being set to zero, the model learns to distribute information across neurons within the layer. Formally, given an intermediate layer's activation vector, dropout applies a binary mask sampled from a Bernoulli distribution to zero out units randomly. During inference, all neurons are used, and their outputs are scaled to match the expected training behavior.
			<br><br>
			<div class="centered-image">
				<img src="./images/dropout.png" width=512px/>
				<figcaption>Figure 3: Dropout in a neural network. Neurons are stochastically deactivated during training. Borrowed figure from <a href="#ref_2">[2]</a>.</figcaption>
			</div>
			<br><br>
			Here we employ dropout within the spectral perspective. Instead of randomly dropping out neurons, we stochastically drop out the spectral components in deep models.
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="spectral_dropout">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h2>Spectral Dropout</h2>
			First, we note that Khan et al. <a href="#ref_3">[3]</a> produced a regularization approach of the same name, but different strategy. They operate by applying a decorrelation transform to neuron activations and dropping out the weak/noisy components in this frequency domain. Their solution improved accuracy on a number of canonical deep learning problems. 
			<br><br>
			Our spectral dropout works by taking a linear layer weight matrix <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math> and stochastically deactivating components in its spectral representation during training. We essentially want to randomly zero out some singular values (thereby removing its associated component) such that we learn adaptable spectral components. Specifically, given some sampling procedure <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>(</mo><mo>)</mo></math> and <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math>, we introduce our algorithm.
			<br><br>
			<div class="centered-image">
				<img src="./images/specdropalgo.png" width=512px/>
				<figcaption>Figure 4: Spectral Dropout Algorithm.</figcaption>
			</div>
			<br><br>
			How does one choose the sampling procedure <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>(</mo><mo>)</mo></math>? Under the heuristic of favoring low-rank matrices, we elected to keep components with probabilities proportional to their singular value. Specifically, we set the drop out probability to <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>(</mo><msub><mi>&#x03C3;</mi><mi>i</mi></msub><mo>,</mo><mi>W</mi><mo>)</mo><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><msub><mi>&#x03C3;</mi><mi>i</mi></msub><mrow><mo>&#x2225;</mo><mi>W</mi><mo>&#x2225;</mo><mo>*</mo></mrow></mfrac></math>. The normalization here is just the sum of singular values. There may be finetuning to be done here, as one could substitute the nuclear norm for the Frobenius norm, or employ an altogether different procedure.
			<br><br>
			<div class="centered-image">
				<img src="./images/specdropoutex.png" width=512px/>
				<figcaption>Figure 5: Sample Spectral Dropout Run. Some components get dropped, others stay.</figcaption>
			</div>
			<br><br>
			To build some intuition with spectral dropout, consider a forward pass through the network where we apply the regularization technique and use modified matrix <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W'</mi><mo>=</mo><mi>U</mi><mi>&#x03A3;'</mi><msup><mi>V</mi><mo>T</mo></msup></math>. If, upon evaluation of the loss function, we have good performance on the objective, then the gradient update will scale up the spectral components which picked up improved features. Conversely, if we have high loss, the gradient update will diminish those less useful spectral components. The randomness in the dropout ensures that we don't get stuck learning in a poor section of the spectral space.
			<br><br>
			The goal of spectral dropout is to sparsify the singular value spectrum in these linear layers via randomness so that we learn highly generalizable features. As in standard dropout, this stochastic constraining of model complexity will mitigate overfitting.
			</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h2>Experiments</h2>
			As a proof of concept, we apply spectral dropout on training a convolutional neural network (CNN) for classifying images in the CIFAR-10 dataset <a href="#ref_4">[4]</a>. This problem consists of building a mapping from color images to one of ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. We took a modified AlexNet <a href="#ref_5">[5]</a> architecture and tested it with various dropout techniques. The structure was comprised of 5 convolutional layers, 3 max-pooling layers, and 2 fully connected layers, and dropout is standardly applied after each linear layer.
			<br><br>
			<div class="centered-image">
				<img src="./images/alexnet_diagram.png" width=784px/>
				<figcaption>Fig. 6. AlexNet architecture. ReLU activation is used after convolutional and linear layers.</figcaption>
			</div>
			<br><br>
			As for hyperparameters, we trained for 6 epochs with batch size 64, using cross-entropy loss (as we have a classification problem) with the SGD optimizer (learning rate 0.005, weight decay 0.005, momentum 0.9). We also follow the protocol to transform CIFAR-10 images from 32x32 to the proper 227x227 input for the unmodified AlexNet architecture.
			<br><br>
			<h3>Control</h3>
					We first measure the performance of AlexNet for the following baselines:
					<ul>
						<li>No dropout (p = 0)</li>
						<li>Standard dropout (p = 0.5)</li>
					</ul>
					<br><br>
					<div class="centered-image">
						<img src="./images/control.png" width=512px/>
						<figcaption>Fig. 7. Training loss for controls, with dropout probability p.</figcaption>
					</div>
					<br><br>
					Final accuracy without dropout was 70.03 percent. With standard dropout it was 71.45 percent. So, we see that dropout does improve generalization and test performance at the cost of more variable and higher training loss.
			<br><br>
			<h3>Test</h3>
			Next we implemented our spectral dropout procedure on the first linear layer after the convolutional section in our CNN.
			<br><br>
			<div class="centered-image">
				<img src="./images/test.png" width=512px/>
				<figcaption>Fig. 8. Training loss for spectral dropout.</figcaption>
			</div>
			<br><br>
			Final accuracy with spectral dropout was 77.26 percent, so spectral dropout reduced overfitting and improved the model. Moreover, the spectral norm of the weight matrix was smaller than those in the control, suggesting a simpler, regularized model. The training process took over 10x as long, however, due to the costly task of performing an SVD with each forward pass. These results demonstrate the efficacy of spectral dropout in regularization.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);">
		    </div>
		</div>

		<div class="content-margin-container" id="next_steps_and_considerations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Next Steps and Considerations</h2>
			Spectral dropout shows promise, but there are still important questions to be answered. Chief among them is how to best choose the sampling procedure <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>(</mo><mo>)</mo></math>. As aforementioned, although we elected to keep spectral components with probabilities proportional to their singular values, there's no reason to say that this is the best method. If one abstracts the training process as searching through a high-dimensional manifold of matrices, we'd ideally like our procedure to ensure that we balance the exploration of that space with exploitation of good solutions. So we expect that adding a hyperparameter which tunes how much spectral dropout upregulates the sampling of smaller spectral components could help the model explore more of that parameter landscape, finding better solutions faster. After all, poor initialization could lead to the right features being locked in small spectral components, so we want to make sure that we find those in training. In general more research needs to be done on the variability of spectral dropout, especially as it relates to this sampling function.
			<br><br>
			A second consideration involves the speed and computational effort of spectral dropout. For a matrix of size <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mo>&#xD7;</mo><mi>n</mi></math>, the computational complexity of an SVD is <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>O</mo><mo>(</mo><mo>min</mo><mo>{</mo><mi>m</mi><msup><mi>n</mi><mn>2</mn></msup><mo>,</mo><msup><mi>m</mi><mn>2</mn></msup><mi>n</mi><mo>}</mo><mo>)</mo></math>, which is quite expensive as we run it on larger and larger linear layers. We may be able to achieve comparable results much faster by reducing <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math> to its top <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math> components and only applying spectral dropout with that simplified weight matrix. Instead of performing the reduced SVD, we can compute the top <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math> singular values and vectors with power iteration on the Gram matrix <math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mo>T</mo></msup><mi>W</mi></math> and then reconstruct <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W'</mi></math> post selection of dropout components.
			</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Conclusion</h2>
			Regularization from the spectral perspective endeavors to improve generalization of deep learning models by simplifying their spectral components. This blog introduces a new regularization technique called spectral dropout, which stochastically removes these components in order to learn highly generalizable features. By randomly dropping out some sections in the singular value spectrum of layer matrices, models learn to develop strong components across the spectrum. 
			<br><br>
			Experimental results of applying this method on classifying CIFAR-10 images demonstrate its potential, as we improved accuracy over standard dropout. Nevertheless, this needs to be weighed against the longer training time and additional hyperparameter finetuning required through selection of the sampling function <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math>.
			<br><br>
			The spectral perspective is particularly useful in understanding matrices, in the sense that we can represent a complex object as a linear combination of several smaller and simpler ones. Thinking about regularization of deep networks under this framework is just as effective, and we hope that spectral dropout adds another weapon to the data scientist's arsenal.
			</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_0"></a>[0] <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition</a>, Wikipedia, 2024.<br><br>
							<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/1705.10941">Spectral Norm Regularization for Improving the Generalizability of Deep Learning</a>, Yoshida, Miyato, 2017.<br><br>
							<a id="ref_2"></a>[2] <a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov, 2014.<br><br>
							<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1711.08591">Regularization of Deep Neural Networks with Spectral Dropout</a>, Khan, Hayat, Porikli, 2017.<br><br>
							<a id="ref_4"></a>[4] <a href="https://www.cs.toronto.edu/~kriz/cifar.html">The CIFAR-10 dataset</a>, Krizhevsky, 2009.<br><br>
							<a id="ref_5"></a>[5] <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, Krizhevsky, Sutskever, Hinton, 2012.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
